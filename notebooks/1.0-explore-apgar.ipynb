{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import os\n",
    "import zipfile\n",
    "import seaborn as sns\n",
    "from multiprocessing import Pool\n",
    "from src.data.data_prep_utils import df_from_csv_no_geo_extra, df_from_csv_no_geo\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Different CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPUs avail: 32\n",
      "MemTotal:       263745244 kB\n",
      "MemFree:        70700172 kB\n",
      "MemAvailable:   227684088 kB\n"
     ]
    }
   ],
   "source": [
    "# to check CPU count\n",
    "import multiprocessing\n",
    "print('CPUs avail:', multiprocessing.cpu_count()) # or os.cpu_count()\n",
    "\n",
    "# memory available\n",
    "# https://stackoverflow.com/a/48140392/9214620\n",
    "!cat /proc/meminfo | grep Mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path.cwd().parent\n",
    "folder_raw_data = root_dir / 'data/raw'\n",
    "folder_processed_data = root_dir / 'data/processed'\n",
    "folder_external_data = root_dir / 'data/external'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of file names\n",
    "files = os.listdir(folder_raw_data)\n",
    "file_list = [folder_raw_data / filename for filename in files if filename.endswith('.csv')]\n",
    "# file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSV is very large, therefore we will only load certain columns. The columns names are listed in the description pdf. These are the ones we will use for **2003-2004**:\n",
    "* **dob_yy**: date of birth year\n",
    "* **dob_mm**: date of birth month\n",
    "* **dob_wk**: date of birth weekday\n",
    "* **mrstate**: mother's resident state\n",
    "* **mrecntyfips**: mother's resident county FIPS code\n",
    "* **mrcityfips**: mother's place of residence (city) FIPS code\n",
    "* **apgar5**: five minute Apgar score\n",
    "* **apgar5r**: five minute Apgar score, recoded\n",
    "\n",
    "Geographic data is not available from **2005 onwards** (see [column description for 2005 on NBER](https://data.nber.org/natality/2005/desc/natl2005/desc.txt)). Therefore, only these columns will be used.\n",
    "* **dob_yy**: date of birth year\n",
    "* **dob_mm**: date of birth month\n",
    "* **dob_wk**: date of birth weekday\n",
    "* **apgar5**: five minute Apgar score\n",
    "* **apgar5r**: five minute Apgar score, recoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the year, there are different naming conventions. Here's the ones we'll use for **1989 through 2002** (see [column description for 1989 on NBER](https://data.nber.org/natality/1989/desc/natl1989/desc.txt)):\n",
    "\n",
    "* **biryr**: date of birth year (can also use 'datayear')\n",
    "* **birmon**: date of birth month\n",
    "* **weekday**: date of birth weekday\n",
    "* **stresfip**: mother's resident state FIPS code\n",
    "* **cntyrfip**: mother's resident county FIPS code\n",
    "* **cityres**: mother's place of residence (city) - unsure if fips code\n",
    "* **fmaps**: five minute Apgar score\n",
    "* **fmapsr**: five minute Apgar score, recoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the ones we'll use for 1982 through 1988 (see [column description on NBER](https://data.nber.org/natality/1988/desc/natl1988/desc.txt)):\n",
    "\n",
    "* **datayear**: date of birth year\n",
    "* **birmon**: date of birth month\n",
    "* **birday**: birth date - day (like the 15th of June) (1968-1988 only)\n",
    "* ~~**weekday**: date of birth weekday~~ (does not exist in this date range)\n",
    "* **stresfip**: mother's resident state FIPS code\n",
    "* **cntyrfip**: mother's resident county FIPS code\n",
    "* **cityres**: mother's place of residence (city) - unsure if fips code\n",
    "* **fmaps**: five minute Apgar score\n",
    "* **fmapsr**: five minute Apgar score, recoded\n",
    "\n",
    "From 1978 onwards, APGAR score is included. (fmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path.cwd().parent\n",
    "folder_raw_data = root_dir / 'data/raw'\n",
    "folder_processed_data = root_dir / 'data/processed'\n",
    "folder_external_data = root_dir / 'data/external'\n",
    "\n",
    "def main(folder_raw_data):\n",
    "    \"\"\"Runs data processing scripts to turn raw data from (../raw) into\n",
    "    cleaned data ready to be analyzed (saved in ../processed).\n",
    "    \"\"\"\n",
    "\n",
    "    # get a list of file names\n",
    "    files = os.listdir(folder_raw_data)\n",
    "    file_list = [\n",
    "        Path(folder_raw_data) / filename\n",
    "        for filename in files\n",
    "        if filename.endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    # set up your pool\n",
    "    with Pool(processes=12) as pool:  # or whatever your hardware can support\n",
    "\n",
    "        # have your pool map the file names to dataframes\n",
    "        df_list = pool.map(df_from_csv_no_geo_extra, file_list)\n",
    "\n",
    "        # reduce the list of dataframes to a single dataframe\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "        return combined_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # not used in this stub but often useful for finding various files\n",
    "    project_dir = Path.cwd().parent\n",
    "    print(type(project_dir))\n",
    "\n",
    "    df = main(project_dir / \"data/raw/\")\n",
    "    print(\"Final df shape:\", df.shape)\n",
    "\n",
    "    df.to_csv(project_dir / \"data/processed\" / \"birth_no_geo_apgar.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix 2019 and 2020 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2019\n",
    "file_natl = folder_raw_data / f'natl{str(year)}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dob_yy</th>\n",
       "      <th>dob_mm</th>\n",
       "      <th>dob_tt</th>\n",
       "      <th>dob_wk</th>\n",
       "      <th>octerr</th>\n",
       "      <th>ocntyfips</th>\n",
       "      <th>ocntypop</th>\n",
       "      <th>bfacil</th>\n",
       "      <th>f_facility</th>\n",
       "      <th>bfacil3</th>\n",
       "      <th>...</th>\n",
       "      <th>ca_cleft</th>\n",
       "      <th>ca_clpal</th>\n",
       "      <th>f_ca_cleft</th>\n",
       "      <th>f_ca_downs</th>\n",
       "      <th>f_ca_chrom</th>\n",
       "      <th>f_ca_hypos</th>\n",
       "      <th>no_congen</th>\n",
       "      <th>itran</th>\n",
       "      <th>ilive</th>\n",
       "      <th>f_bfed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>01</td>\n",
       "      <td>1123</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 224 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  dob_yy dob_mm dob_tt dob_wk octerr ocntyfips ocntypop bfacil f_facility  \\\n",
       "0   2020     01   1123      4    NaN       NaN      NaN      1          1   \n",
       "\n",
       "  bfacil3  ... ca_cleft ca_clpal f_ca_cleft f_ca_downs f_ca_chrom f_ca_hypos  \\\n",
       "0       1  ...        N        N          1          1          1          1   \n",
       "\n",
       "  no_congen itran ilive f_bfed  \n",
       "0         1     N     Y      1  \n",
       "\n",
       "[1 rows x 224 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file_natl, nrows=1, dtype=str)\n",
    "df.columns = df.columns.str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 processing complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dob_yy</th>\n",
       "      <th>dob_mm</th>\n",
       "      <th>apgar5</th>\n",
       "      <th>births</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dob_yy  dob_mm  apgar5  births\n",
       "0    2019       1       8       2\n",
       "1    2019       1       9       1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_csv_no_geo_extra(file_natl, nrows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 processing complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dob_yy</th>\n",
       "      <th>dob_mm</th>\n",
       "      <th>births</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dob_yy  dob_mm  births\n",
       "0    2019       1       3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_csv_no_geo(file_natl, nrows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Previously Created Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dob_yy</th>\n",
       "      <th>dob_mm</th>\n",
       "      <th>apgar5</th>\n",
       "      <th>births</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>1995</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>1498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>1995</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>1995</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>18047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6046</th>\n",
       "      <td>1995</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>207028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6047</th>\n",
       "      <td>1995</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>78831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6048 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dob_yy  dob_mm  apgar5  births\n",
       "0       2011       1       0     167\n",
       "1       2011       1       1     720\n",
       "2       2011       1      10   10864\n",
       "3       2011       1       2     492\n",
       "4       2011       1       3     586\n",
       "...      ...     ...     ...     ...\n",
       "6043    1995       9       6    1498\n",
       "6044    1995       9       7    3924\n",
       "6045    1995       9       8   18047\n",
       "6046    1995       9       9  207028\n",
       "6047    1995       9      99   78831\n",
       "\n",
       "[6048 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = Path.cwd().parent\n",
    "folder_processed_data = root_dir / 'data/processed'\n",
    "\n",
    "df = pd.read_csv(folder_processed_data /\"birth_no_geo_apgar.csv\")\n",
    "# df = df[df['dob_yy'].isin([1978, 1979])]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['apgar5']!=99]\n",
    "df['apgar5'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = df.groupby(['dob_yy', 'apgar5']).agg({'births':'sum'})\n",
    "df_count = df_count.groupby(level=0).apply(lambda x: 100 * x / x.sum())\n",
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = df_count.reset_index().rename(columns={'births':'births_pct'})\n",
    "df_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the apgar5 distribution by year\n",
    "df_a = df_count[df_count['apgar5']==1]\n",
    "\n",
    "plt.plot(df_a['dob_yy'], df_a['births_pct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the \"births_pct\" distribution by year all on the same plot\n",
    "df_b = df_count[df_count['apgar5']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.sort(df_count['dob_yy'].unique())\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "for year in np.sort(df_count['dob_yy'].unique()):\n",
    "    df_year = df_count[(df_count['dob_yy']==year) & (df_count['apgar5']<3) \n",
    "    # & (df_count['apgar5']<6)\n",
    "    ]\n",
    "    ax.scatter(df_year['apgar5'], df_year['births_pct'], label=year)\n",
    "\n",
    "# set y-axis to log scale\n",
    "ax.set_yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_count[df_count['dob_yy']==2008]['apgar5'], df_count[df_count['dob_yy']==2008]['births_pct'],)\n",
    "plt.plot(df_count[df_count['dob_yy']==2009]['apgar5'], df_count[df_count['dob_yy']==2009]['births_pct'],)\n",
    "\n",
    "# change the y-axis scale to log\n",
    "plt.yscale('log')\n",
    "# plt.plot(df_count[df_count['dob_yy']==2009]['births_pct'], df_count[df_count['dob_yy']==2009]['apgar5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data with Percentage by Month\n",
    "Decent article on groupby: https://towardsdatascience.com/data-grouping-in-python-d64f1203f8d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path.cwd().parent\n",
    "folder_processed_data = root_dir / 'data/processed'\n",
    "\n",
    "df = pd.read_csv(folder_processed_data /\"birth_no_geo_apgar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['apgar5']!=99].sort_values(by=['dob_yy', 'dob_mm'])\n",
    "df = df.groupby(['dob_yy', 'dob_mm', 'apgar5']).agg({'births':'sum'}).reset_index()\n",
    "df['yy_mm_births_total'] = df.groupby(['dob_yy', 'dob_mm'])['births'].transform('sum')\n",
    "df['birth_pct'] = df['births'] / df['yy_mm_births_total'] * 100\n",
    "df[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we best see if the apgar score is changing over time?\n",
    "* Heatmap of the apgar score by year or month?\n",
    "* Should we measure the change in apgar score as a percentage of change over some average?\n",
    "\n",
    "Let's start working on the heatmap.\n",
    "\n",
    "## Heatmap Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path.cwd().parent\n",
    "folder_processed_data = root_dir / 'data/processed'\n",
    "\n",
    "df = pd.read_csv(folder_processed_data /\"birth_no_geo_apgar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['apgar5']!=99].sort_values(by=['dob_yy', 'dob_mm'])\n",
    "df = df.groupby(['dob_yy', 'dob_mm', 'apgar5']).agg({'births':'sum'}).reset_index()\n",
    "df['yy_mm_births_total'] = df.groupby(['dob_yy', 'dob_mm'])['births'].transform('sum')\n",
    "df['births_pct'] = df['births'] / df['yy_mm_births_total'] * 100\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['apgar5']==8) & (df['dob_yy']==2008)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df[df['apgar5'] == 10][['dob_yy', 'dob_mm', 'births_pct']].pivot('dob_yy', 'dob_mm', 'births_pct'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch\n",
    "https://www.kite.com/python/answers/how-to-generate-percentages-of-pandas-columns-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = df.groupby(['dob_yy', 'dob_mm', 'apgar5'], as_index=False).agg({'births': 'sum'})\n",
    "dfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_perc = dfp.groupby(level=0,).apply(lambda x : 100.0 * x / x.sum())\n",
    "dp_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_perc[dp_perc['dob_yy']==1978]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = df.describe()\n",
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(df['dob_yy'].unique()).sort()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 1968\n",
    "\n",
    "df = df_from_csv_no_geo_extra(folder_raw_data / f'natl{str(year)}.csv', nrows=2000)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['dob_yy']==1989) & (df['apgar5']==1) & (df['dob_mm']==1)]['apgar5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"dob_yy\", \"dob_mm\", \"apgar5\", \"births\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns for 2003 through 2004\n",
    "col_load_1 = ['dob_yy','dob_mm','dob_wk','mrstate','mrcntyfips','mrcityfips', 'apgar5']\n",
    "col_1_dtype = [int, int, int, str, int, int, int]\n",
    "\n",
    "# columns for 1989 through 2002\n",
    "col_load_2 =['biryr', 'birmon', 'weekday', 'stresfip', 'cntyrfip', 'cityres']\n",
    "\n",
    "# columns for 2005+\n",
    "col_load_3 =['dob_yy','dob_mm','dob_wk']\n",
    "\n",
    "# columns for 1982 through 1988\n",
    "col_load_4 =['datayear', 'birmon','birday','stresfip', 'cntyrfip', 'cityres']\n",
    "rename_col4 = ['dob_yy','dob_mm','dob_day','mrstate','mrcntyfips','mrcityfips',]\n",
    "\n",
    "# create dictionary to rename older csvs\n",
    "col_rename_dict = dict(zip(col_load_2, col_load_1))\n",
    "col_rename_dict4 = dict(zip(col_load_4, rename_col4))\n",
    "col_rename_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load csv from 1972. This does not have any geographic data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 1972\n",
    "df = pd.read_csv(folder_raw_data / f'natl{str(year)}.csv', nrows=1, usecols=['datayear', 'birmon','birday',], dtype=str).rename(columns=col_rename_dict4)\n",
    "# df = df.rename(columns={'mrstate':'mrstatefips'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load \"older\" csv (from 1991-2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 1993\n",
    "df = pd.read_csv(folder_raw_data / f'natl{str(year)}.csv', nrows=1, usecols=col_load_2, dtype=str).rename(columns=col_rename_dict)\n",
    "df = df.rename(columns={'mrstate':'mrstatefips'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 'all-geocodes-v2017.xlsx'\n",
    "# df_fips = pd.read_csv('./other_data/all-geocodes-v2017.csv', \n",
    "#                         dtype={'State Code (FIPS)': int, \n",
    "#                                'County Code (FIPS)': int, \n",
    "#                                'County Subdivision Code (FIPS)': int, \n",
    "#                                'Place Code (FIPS)': int, \n",
    "#                                'Consolidtated City Code (FIPS)': int})\n",
    "\n",
    "df_fips = pd.read_csv(folder_external_data / 'all-geocodes-v2017.csv', dtype=str)\n",
    "\n",
    "df_fips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the fips codes for the states only\n",
    "# df_state_fips = df_fips[(df_fips['State Code (FIPS)']>0) & \n",
    "#                         (df_fips['County Code (FIPS)']==0) & \n",
    "#                         (df_fips['County Subdivision Code (FIPS)']==0) & \n",
    "#                         (df_fips['Place Code (FIPS)']==0) & \n",
    "#                         (df_fips['Consolidtated City Code (FIPS)']==0)\n",
    "#                        ][['State Code (FIPS)','Area Name (including legal/statistical area description)']]\n",
    "\n",
    "\n",
    "df_state_fips = df_fips[(df_fips['County Code (FIPS)']=='000') & \n",
    "                        (df_fips['County Subdivision Code (FIPS)']=='00000') & \n",
    "                        (df_fips['Place Code (FIPS)']=='00000') & \n",
    "                        (df_fips['Consolidtated City Code (FIPS)']=='00000')\n",
    "                       ][['State Code (FIPS)','Area Name (including legal/statistical area description)']]\n",
    "\n",
    "# rename columns in df\n",
    "df_state_fips.columns = ['state_fips', 'state_name_mr']\n",
    "df_state_fips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_state_fips, left_on='mrstatefips',\n",
    "              right_on='state_fips', how='inner', copy=False).drop(['state_fips'], axis=1).drop(['mrstatefips'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the dtype for the numerical columns\n",
    "df = df.astype({'dob_mm':int, 'dob_wk':int, 'dob_yy':int})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 1982-1988."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 1982\n",
    "df = pd.read_csv(folder_raw_data / f'natl{str(year)}.csv', nrows=1, usecols=col_load_4, dtype=str).rename(columns=col_rename_dict4)\n",
    "df = df.rename(columns={'mrstate':'mrstatefips'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_state_fips, left_on='mrstatefips',\n",
    "              right_on='state_fips', how='inner', copy=False).drop(['state_fips'], axis=1).drop(['mrstatefips'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 2005+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2007\n",
    "\n",
    "# columns for 2005+\n",
    "col_load_3 =['dob_yy','dob_mm','dob_wk',]\n",
    "# col_load_3 =['dob_yy','dob_mm','dob_wk', 'ocntyfips']\n",
    "\n",
    "df = pd.read_csv(folder_raw_data / f'natl{str(year)}.csv', nrows=1, usecols=col_load_3, dtype=str)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 2003-2004 csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2003\n",
    "df = pd.read_csv(folder_raw_data / f'natl{str(year)}.csv', nrows=1, usecols=col_load_1, dtype=str)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 'state_abbreviations.csv'\n",
    "df_abbr = pd.read_csv(folder_external_data / 'state_abbreviations.csv',header=None, names=['state','abbr'])\n",
    "df_abbr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas merge. Also, dorp the 'abbr' columns right away\n",
    "df = pd.merge(df, df_abbr, left_on='mrstate',right_on='abbr', how='inner', copy=False).drop(['abbr'], axis=1).drop(['mrstate'], axis=1)\n",
    "df = df.rename(columns={'state':'state_name_mr'})\n",
    "df = df.astype({'dob_mm':int, 'dob_wk':int, 'dob_yy':int})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add state fips code\n",
    "df = pd.merge(df, df_state_fips, left_on='state_name_mr',\n",
    "              right_on='state_name_mr', how='inner', copy=False)\n",
    "df = df.rename(columns={'state_fips':'mrstatefips'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice grouping a table together by birth year/month/week-day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2005\n",
    "\n",
    "# columns for 2005+\n",
    "col_load_3 =['dob_yy','dob_mm','dob_wk',]\n",
    "\n",
    "df = pd.read_csv(folder_raw_data / f'natl{str(year)}.csv', nrows=100, usecols=col_load_3, dtype=int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['births'] = np.ones(df.shape[0])\n",
    "df1 = df.groupby(['dob_yy', 'dob_mm', 'dob_wk'], as_index=False).count().sort_values(by=['dob_yy','dob_mm','dob_wk'])\n",
    "df1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataframe is much smaller - only 84 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "root_dir = Path.cwd().parent\n",
    "data_file = \"birth_geo_births_ind_test.csv.gz\"\n",
    "\n",
    "dtype_dict = {'dob_yy':int, 'dob_mm':int, 'mrcntyfips':str, 'mrcityfips':str, 'state_name_mr':str,\n",
    "       'mrstatefips':int, 'apgar5':int, 'births':int}\n",
    "\n",
    "df = pd.read_csv(root_dir / data_file, compression='gzip', dtype=dtype_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(df['dob_yy'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"birth_geo_births_summed_test.csv.gz\"\n",
    "\n",
    "dtype_dict = {'dob_yy':int, 'dob_mm':int, 'mrcntyfips':str, 'mrcityfips':str, 'state_name_mr':str,\n",
    "       'mrstatefips':int, 'apgar5':int, 'births':int}\n",
    "\n",
    "df = pd.read_csv(root_dir / data_file, compression='gzip', dtype=dtype_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
